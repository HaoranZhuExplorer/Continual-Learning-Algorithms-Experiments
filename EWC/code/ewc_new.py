# -*- coding: utf-8 -*-
"""EWC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12cvrWSH0i5LYE8c4ATDfMP-g02fd5c0t
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch import autograd
import numpy as np
from torch.utils.data import DataLoader


class ElasticWeightConsolidation:

    def __init__(self, model, crit, lr=0.001, weight=1000000):
        self.model = model
        self.weight = weight
        self.crit = crit
        self.optimizer = optim.Adam(self.model.parameters(), lr)



    def _update_mean_params(self):
        for param_name, param in self.model.named_parameters():
            print(param_name)
            _buff_param_name = param_name.replace('.', '__')
            print(_buff_param_name)
            self.model.register_buffer(_buff_param_name+'_estimated_mean', param.data.clone())

    def _update_fisher_params(self, current_ds, batch_size, num_batch):
        dl = DataLoader(current_ds, batch_size, shuffle=True)
        log_liklihoods = []
        for i, (input, target) in enumerate(dl):
            if i > num_batch:
                break
            output = F.log_softmax(self.model(input), dim=1)
            log_liklihoods.append(output[:, target])
        log_likelihood = torch.cat(log_liklihoods).mean()
        grad_log_liklihood = autograd.grad(log_likelihood, self.model.parameters())
        _buff_param_names = [param[0].replace('.', '__') for param in self.model.named_parameters()]
        for _buff_param_name, param in zip(_buff_param_names, grad_log_liklihood):
            self.model.register_buffer(_buff_param_name+'_estimated_fisher', param.data.clone() ** 2)

    def register_ewc_params(self, dataset, batch_size, num_batches):
        self._update_fisher_params(dataset, batch_size, num_batches)
        self._update_mean_params()

    def _compute_consolidation_loss(self, weight):
        try:
            losses = []
            for param_name, param in self.model.named_parameters():

                _buff_param_name = param_name.replace('.', '__')

                estimated_mean = getattr(self.model, '{}_estimated_mean'.format(_buff_param_name))

                estimated_fisher = getattr(self.model, '{}_estimated_fisher'.format(_buff_param_name))
                losses.append((estimated_fisher * (param - estimated_mean) ** 2).sum())
            return (weight / 2) * sum(losses)
        except AttributeError:
            return 0

    def forward_backward_update(self, input, target):
        output = self.model(input) 
        loss = self._compute_consolidation_loss(self.weight) + self.crit(output, target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def save(self, filename):
        torch.save(self.model, filename)

    def load(self, filename):
        self.model = torch.load(filename)

import torch
import torch.nn as nn
import torch.optim as optim


# hyperparameters
# loss function criterion
criterion = nn.CrossEntropyLoss()
# learning rate
lr = 0.001
# set how important old task is to new task
old_new_rate = 10000   
# optimizer
optimizer_name = "Adam"


class EWC:
  def __init__(self, model, old_new_rate):
    self.model  = model
    self.old_new_rate = old_new_rate
    self.approximate_mean = 0
    self.approximate_fisher_information_matrix = 0


  def get_old_task_loss():
      print(self.model)
      return
      losses = []
      for param_name, param in self.model.named_parameters():
        losses.append((self.approximate_fisher_information_matrix * (param - self.approximate_mean) ** 2).sum())
        return (old_new_rate / 2) * sum(losses) 

  def train(self, data, target):
    if optimizer_name =="Adam":
      optimizer = optim.Adam(self.model.parameters(), lr=lr)
      output = self.model(data)

      optimizer.zero_grad()
      loss_new_task = criterion(output, target)
      loss_old_task = 0
      loss = loss_new_task + loss_old_task
      loss.backward()
      optimizer.step()


  # update approximate mean and fisher information matrix
  def update(self):
    return

# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OXMaQKv-tijixj9j1uqcI3C9n18h_qlV
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, transforms

mnist_train = datasets.MNIST("../data", train=True, download=True, transform=transforms.ToTensor())
mnist_test = datasets.MNIST("../data", train=False, download=True, transform=transforms.ToTensor())
train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)
test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)

def accu(model, dataloader):
    model = model.eval()
    acc = 0
    for input, target in dataloader:
        o = model(input)
        acc += (o.argmax(dim=1).long() == target).float().mean()
    return acc / len(dataloader)


class LinearLayer(nn.Module):
    def __init__(self, input_dim, output_dim, act='relu', use_bn=False):
        super(LinearLayer, self).__init__()
        self.use_bn = use_bn
        self.lin = nn.Linear(input_dim, output_dim)
        self.act = nn.ReLU() if act == 'relu' else act
        if use_bn:
            self.bn = nn.BatchNorm1d(output_dim)
    def forward(self, x):
        if self.use_bn:
            return self.bn(self.act(self.lin(x)))
        return self.act(self.lin(x))

class Flatten(nn.Module):

    def forward(self, x):
        return x.view(x.shape[0], -1)

class BaseModel(nn.Module):
    
    def __init__(self, num_inputs, num_hidden, num_outputs):
        super(BaseModel, self).__init__()
        self.f1 = Flatten()
        self.lin1 = LinearLayer(num_inputs, num_hidden, use_bn=True)
        self.lin2 = LinearLayer(num_hidden, num_hidden, use_bn=True)
        self.lin3 = nn.Linear(num_hidden, num_outputs)
        
    def forward(self, x):
        return self.lin3(self.lin2(self.lin1(self.f1(x))))

crit = nn.CrossEntropyLoss()

ewc = EWC(model=BaseModel(28 * 28, 100, 10), old_new_rate=old_new_rate)

print("start training")
for epoch in range(0, 4):
    print("epoch: "+str(epoch))
    for data, target in train_loader:
        ewc.train(data, target)
    acc0 = accu(ewc.model, test_loader)
    print("task1: "+str(acc0))
print("\n")

ewc.update()
acc0 = accu(ewc.model, test_loader)
print("task1, time 1: "+str(acc0))


f_mnist_train = datasets.FashionMNIST("../data", train=True, download=True, transform=transforms.ToTensor())
f_mnist_test = datasets.FashionMNIST("../data", train=False, download=True, transform=transforms.ToTensor())
f_train_loader = DataLoader(f_mnist_train, batch_size = 100, shuffle=True)
f_test_loader = DataLoader(f_mnist_test, batch_size = 100, shuffle=False)

print("start training")
for epoch in range(0, 4):
    for data, target in (f_train_loader):
        ewc.train(data, target)
print("\n")

ewc.update()



acc1 = accu(ewc.model, f_test_loader)
acc2 = accu(ewc.model, test_loader)
print("task2, time 2: "+str(acc1))
print("task1, time 1: "+str(acc2))