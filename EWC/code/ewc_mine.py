
# -*- coding: utf-8 -*-
"""EWC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12cvrWSH0i5LYE8c4ATDfMP-g02fd5c0t
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch import autograd

# hyperparameters
# loss function criterion
criterion = nn.NLLLoss()
# learning rate
lr = 0.001
# set how important old task is to new task
old_new_rate = 10000   
# optimizer
optimizer_name = "Adam"
epoch = 1
train_batch_size = 64
device = "cuda"
log_interval = 1000
max_iter_per_epoch = 100

class EWC:
  # initialize parameters
  def __init__(self, model, old_new_rate):
    self.model  = model.to(device)
    self.old_new_rate = old_new_rate
    self.approximate_mean = 0
    self.approximate_fisher_information_matrix = 0
  
  # function to compute loss regarding to previous task, use an approximate mean and fisher matrix to simplify compute
  def get_old_task_loss(self):
    try:
      losses = []
      for param_name, param in self.model.named_parameters():

        _buff_param_name = param_name.replace('.', '__')        
        estimated_mean = getattr(self.model, '{}_estimated_mean'.format(_buff_param_name))
        estimated_fisher = getattr(self.model, '{}_estimated_fisher'.format(_buff_param_name))
        losses.append((estimated_fisher * (param - estimated_mean) ** 2).sum())
      return (old_new_rate / 2) * sum(losses) 
    except Exception:
      return 0


  # training given model with data
  def train(self, data, target):
    if optimizer_name =="Adam":
      optimizer = optim.Adam(self.model.parameters(), lr=lr)
      output = self.model(data).to(device)

      optimizer.zero_grad()
      loss_new_task = criterion(output, target)
      loss_old_task = self.get_old_task_loss()
      loss = loss_new_task + loss_old_task
      loss.backward()
      optimizer.step()

  # update approximate mean and fisher information matrix
  # use this function after training is over
  def update(self, current_ds, batch_size, num_batch):
    # update approximate mean
    for param_name, param in self.model.named_parameters():
            _buff_param_name = param_name.replace('.', '__')
            self.model.register_buffer(_buff_param_name+'_estimated_mean', param.data.clone())
    
    # update approximate fisher information matrix
    dl = DataLoader(current_ds, batch_size, shuffle=True)
    log_liklihoods = []
    for i, (input, target) in enumerate(dl):
        if i > num_batch:
              break
        input = input.to(device)
        target = target.to(device)
        self.model = self.model.to(device)
        output = F.log_softmax(self.model(input), dim=1)
        log_liklihoods.append(output[:, target])
    log_likelihood = torch.cat(log_liklihoods).mean()
    grad_log_liklihood = autograd.grad(log_likelihood, self.model.parameters())
    _buff_param_names = [param[0].replace('.', '__') for param in self.model.named_parameters()]
    for _buff_param_name, param in zip(_buff_param_names, grad_log_liklihood):
        self.model.register_buffer(_buff_param_name+'_estimated_fisher', param.data.clone() ** 2)

# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OXMaQKv-tijixj9j1uqcI3C9n18h_qlV
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, transforms

mnist_train = datasets.MNIST("../data", train=True, download=True, transform=transforms.ToTensor())
mnist_test = datasets.MNIST("../data", train=False, download=True, transform=transforms.ToTensor())
train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)
test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)


def accu(model, dataloader):
    model = model.eval()
    acc = 0
    for input, target in dataloader:
        o = model(input)
        acc += (o.argmax(dim=1).long() == target).float().mean()
    return acc / len(dataloader)

def test(model, test_loader):
    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()
    print("accuracy:"+str(100. * correct / len(test_loader.dataset))+"%")

           

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

class LinearLayer(nn.Module):
    def __init__(self, input_dim, output_dim, act='relu', use_bn=False):
        super(LinearLayer, self).__init__()
        self.use_bn = use_bn
        self.lin = nn.Linear(input_dim, output_dim)
        self.act = nn.ReLU() if act == 'relu' else act
        if use_bn:
            self.bn = nn.BatchNorm1d(output_dim)
    def forward(self, x):
        if self.use_bn:
            return self.bn(self.act(self.lin(x)))
        return self.act(self.lin(x))

class Flatten(nn.Module):

    def forward(self, x):
        return x.view(x.shape[0], -1)

class BaseModel(nn.Module):
    
    def __init__(self, num_inputs, num_hidden, num_outputs):
        super(BaseModel, self).__init__()
        self.f1 = Flatten()
        self.lin1 = LinearLayer(num_inputs, num_hidden, use_bn=True)
        self.lin2 = LinearLayer(num_hidden, num_hidden, use_bn=True)
        self.lin3 = nn.Linear(num_hidden, num_outputs)
        
    def forward(self, x):
        return self.lin3(self.lin2(self.lin1(self.f1(x))))

crit = nn.CrossEntropyLoss()

ewc = EWC(model=BaseModel(28 * 28, 100, 10), old_new_rate=old_new_rate)

#ewc = EWC(model=Net().to(device), old_new_rate=old_new_rate)

print("start training")
for i in range(0, epoch):
    print("epoch: "+str(i))
    t = 0
    for data, target in train_loader:
        t +=1
        print(t)
        if t==max_iter_per_epoch:
          break
        data, target = data.to(device), target.to(device)
        ewc.train(data, target)
        test(ewc.model, test_loader)
        if t%log_interval==100:
          print(t)
          
print("\n")

mnist_train_size = len(mnist_train)
mnist_batch_num = int(mnist_train_size/train_batch_size)/2
ewc.update(mnist_train, train_batch_size, mnist_batch_num)
test(ewc.model, test_loader)


f_mnist_train = datasets.FashionMNIST("../data", train=True, download=True, transform=transforms.ToTensor())
f_mnist_test = datasets.FashionMNIST("../data", train=False, download=True, transform=transforms.ToTensor())
f_train_loader = DataLoader(f_mnist_train, batch_size = 100, shuffle=True)
f_test_loader = DataLoader(f_mnist_test, batch_size = 100, shuffle=False)

print("start training")
for i in range(0, epoch):
    print("epoch: "+str(i))
    t = 0
    for data, target in (f_train_loader):
        t +=1
        print(t)
        if t==max_iter_per_epoch:
          break
        data, target = data.to(device), target.to(device)
        ewc.train(data, target)
print("\n")

f_mnist_train_sz = len(f_mnist_train)/2
f_batch_num = int(f_mnist_train_sz/train_batch_size)
ewc.update(f_mnist_train, train_batch_size, f_batch_num)



test(ewc.model, f_test_loader)
test(ewc.model, test_loader)